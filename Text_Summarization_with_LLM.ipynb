{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BART-Large-CNN Text Summarization\n",
        "\n",
        "The facebook/bart-large-cnn model uses a sophisticated approach to determine what text is important in a document. Here's a breakdown of how it works:\n",
        "\n",
        "##Architecture and Training\n",
        "BART (Bidirectional and Auto-Regressive Transformers) combines the bidirectional encoding of **BERT** with the autoregressive decoding of **GPT**. It's essentially a sequence-to-sequence model with:\n",
        "\n",
        "**Encoder:** Processes the entire input text bidirectionally (looking at words in context of both what comes before and after)\n",
        "**Decoder:** Generates the summary one token at a time\n",
        "\n",
        "The \"CNN\" in the model name refers to the CNN/Daily Mail dataset it was fine-tuned on, which contains news articles paired with human-written summaries.\n",
        "\n",
        "##How It Determines Important Information\n",
        "BART-large-CNN identifies important content through several mechanisms:\n",
        "\n",
        "**1. Attention Mechanisms**\n",
        "The model uses self-attention to weigh the importance of different words relative to each other. This allows it to:\n",
        "\n",
        "Identify which words are most referenced by other parts of the text\n",
        "Understand relationships between entities and concepts\n",
        "Focus on sentences with high information density\n",
        "\n",
        "**2. Learned Patterns from News Articles**\n",
        "Since it was trained on CNN/Daily Mail news articles, the model has learned journalistic patterns:\n",
        "\n",
        "Important information tends to appear in the opening paragraphs\n",
        "Key information usually answers who, what, when, where, why questions\n",
        "Names, dates, places, and numeric facts often deserve emphasis\n",
        "\n",
        "**3. Content Density Recognition**\n",
        "The model has learned to identify sentences with high information content by recognizing:\n",
        "\n",
        "Higher concentration of named entities (people, places, organizations)\n",
        "Statistical importance of terms (similar to TF-IDF patterns)\n",
        "Sentences that contain new information not redundant with other parts\n",
        "\n",
        "**4. Document Structure Understanding**\n",
        "BART can recognize structural importance:\n",
        "\n",
        "Section headings and topic sentences usually contain important content\n",
        "Conclusion sentences often summarize key points\n",
        "Repetition of concepts across the document signals importance\n",
        "\n",
        "##The Summarization Process\n",
        "\n",
        "**Encoding:** The document is processed through the encoder, which builds semantic representations of each token in context\n",
        "\n",
        "**Importance Weighting:** Through attention mechanisms, the model assigns importance weights to various parts of the text\n",
        "\n",
        "**Content Selection:** The decoder selects the most salient information to include\n",
        "\n",
        "**Generation:** The decoder generates a fluent summary that preserves the key points\n",
        "\n",
        "Unlike extractive summarization (which just pulls out existing sentences), BART performs abstractive summarization, meaning it can rephrase and combine information in new ways.\n",
        "The model doesn't simply extract the most important sentences - it understands concepts and can express them in different words, making the summaries more concise and fluid than simple extraction would allow."
      ],
      "metadata": {
        "id": "wruJeSc3OzpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation"
      ],
      "metadata": {
        "id": "WCF7IU8OO_4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6WaRMHODYpx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from typing import List, Dict, Any, Optional\n",
        "import textwrap\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\"):\n",
        "        \"\"\"\n",
        "        Initialize the text summarizer with a Hugging Face model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the Hugging Face model to use.\n",
        "                Default: \"facebook/bart-large-cnn\" (good balance of quality and speed)\n",
        "                Other options:\n",
        "                - \"t5-small\" (faster, lower quality)\n",
        "                - \"google/pegasus-xsum\" (news-focused)\n",
        "                - \"philschmid/bart-large-cnn-samsum\" (optimized for dialogues)\n",
        "        \"\"\"\n",
        "        print(f\"Loading model: {model_name} (this may take a minute on first run)\")\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "            # For advanced summarization\n",
        "            self.summarization_pipeline = pipeline(\n",
        "                \"summarization\",\n",
        "                model=model_name,\n",
        "                device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "            )\n",
        "\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            print(f\"Model loaded successfully. Using device: {self.device}\")\n",
        "\n",
        "            # Store model name for reference\n",
        "            self.model_name = model_name\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            print(\"Trying to load a smaller model as fallback...\")\n",
        "            try:\n",
        "                # Fallback to a smaller model\n",
        "                fallback_model = \"t5-small\"\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n",
        "                self.model = AutoModelForSeq2SeqLM.from_pretrained(fallback_model)\n",
        "                self.summarization_pipeline = pipeline(\n",
        "                    \"summarization\",\n",
        "                    model=fallback_model,\n",
        "                    device=0 if torch.cuda.is_available() else -1\n",
        "                )\n",
        "                self.model_name = fallback_model\n",
        "                print(f\"Fallback model loaded: {fallback_model}\")\n",
        "            except Exception as e2:\n",
        "                raise RuntimeError(f\"Failed to load both primary and fallback models: {e2}\")\n",
        "\n",
        "    def summarize(self, text: str, max_length: int = 150, min_length: int = 30,\n",
        "                  style: str = \"concise\", preserve_key_points: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Summarize the given text using the loaded model.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to summarize.\n",
        "            max_length (int): Maximum token length for the summary.\n",
        "            min_length (int): Minimum token length for the summary.\n",
        "            style (str): Style of summary (\"concise\", \"detailed\", \"bullet-points\").\n",
        "            preserve_key_points (bool): Whether to try to preserve key information.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated summary.\n",
        "        \"\"\"\n",
        "        # Adjust parameters based on style\n",
        "        if style == \"concise\":\n",
        "            # Keep defaults, shorter summary\n",
        "            pass\n",
        "        elif style == \"detailed\":\n",
        "            # Increase length for more details\n",
        "            max_length = max(max_length * 2, 250)\n",
        "            min_length = max(min_length * 2, 100)\n",
        "        elif style == \"bullet-points\":\n",
        "            # Will be post-processed into bullet points\n",
        "            pass\n",
        "\n",
        "        # For more control, directly set parameters for pipeline\n",
        "        num_beams = 4\n",
        "        if preserve_key_points:\n",
        "            # Using more beams and lower temperature helps preserve key information\n",
        "            num_beams = 6\n",
        "            temperature = 0.7\n",
        "        else:\n",
        "            temperature = 1.0\n",
        "\n",
        "        # Handle long texts by chunking if needed\n",
        "        max_input_length = self.tokenizer.model_max_length\n",
        "        input_ids = self.tokenizer.encode(text, truncation=False)\n",
        "\n",
        "        if len(input_ids) > max_input_length:\n",
        "            # Text is too long, need to chunk it\n",
        "            return self._summarize_long_text(text, max_length, min_length, num_beams, temperature)\n",
        "\n",
        "        # Generate summary\n",
        "        summary = self.summarization_pipeline(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        result = summary[0]['summary_text']\n",
        "\n",
        "        # Post-process for bullet points if requested\n",
        "        if style == \"bullet-points\":\n",
        "            # Convert to bullet points by identifying key sentences\n",
        "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
        "            bullet_points = \"\\n\".join([f\"• {s}.\" for s in sentences])\n",
        "            return bullet_points\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _summarize_long_text(self, text: str, max_length: int, min_length: int,\n",
        "                             num_beams: int, temperature: float) -> str:\n",
        "        \"\"\"\n",
        "        Handle summarization of texts longer than the model's maximum input length.\n",
        "        Uses a chunking approach to process long documents.\n",
        "\n",
        "        Args:\n",
        "            text (str): The long text to summarize.\n",
        "            max_length (int): Maximum token length for the final summary.\n",
        "            min_length (int): Minimum token length for the final summary.\n",
        "            num_beams (int): Number of beams for beam search.\n",
        "            temperature (float): Temperature for generation.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated summary.\n",
        "        \"\"\"\n",
        "        # Split text into sentences\n",
        "        sentences = [s.strip() + '.' for s in text.replace('\\n', ' ').split('.') if s.strip()]\n",
        "\n",
        "        # Calculate max tokens for model\n",
        "        max_tokens = self.tokenizer.model_max_length - 50  # Reserve tokens for generation\n",
        "\n",
        "        # Create chunks that fit within model's context window\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = len(self.tokenizer.encode(sentence))\n",
        "\n",
        "            if current_length + sentence_tokens > max_tokens:\n",
        "                # This chunk is full, start a new one\n",
        "                if current_chunk:  # Make sure we don't add empty chunks\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_tokens\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += sentence_tokens\n",
        "\n",
        "        # Add the last chunk if it's not empty\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        # Summarize each chunk\n",
        "        chunk_summaries = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "            chunk_summary = self.summarization_pipeline(\n",
        "                chunk,\n",
        "                max_length=max(30, max_length // len(chunks)),  # Scale based on chunks\n",
        "                min_length=min(min_length // len(chunks), 20),  # Ensure some minimum\n",
        "                num_beams=num_beams,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            chunk_summaries.append(chunk_summary[0]['summary_text'])\n",
        "\n",
        "        # If we have multiple chunk summaries, summarize them together\n",
        "        if len(chunk_summaries) > 1:\n",
        "            combined_summary = \" \".join(chunk_summaries)\n",
        "\n",
        "            # Check if the combined summary is still too long\n",
        "            if len(self.tokenizer.encode(combined_summary)) > max_tokens:\n",
        "                # Recursively summarize the combined summaries\n",
        "                return self._summarize_long_text(\n",
        "                    combined_summary, max_length, min_length, num_beams, temperature\n",
        "                )\n",
        "            else:\n",
        "                # Final summary of the combined chunk summaries\n",
        "                final_summary = self.summarization_pipeline(\n",
        "                    combined_summary,\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    num_beams=num_beams,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                return final_summary[0]['summary_text']\n",
        "        else:\n",
        "            # If we only had one chunk, its summary is the final summary\n",
        "            return chunk_summaries[0]\n",
        "\n",
        "    def batch_summarize(self, texts: List[str], **kwargs) -> List[str]:\n",
        "        \"\"\"\n",
        "        Summarize multiple texts at once.\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): List of texts to summarize.\n",
        "            **kwargs: Additional arguments passed to summarize method.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of summaries.\n",
        "        \"\"\"\n",
        "        return [self.summarize(text, **kwargs) for text in texts]\n",
        "\n",
        "    def evaluate_summary(self, original_text: str, summary: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the quality of a summary compared to the original text using ROUGE metrics.\n",
        "\n",
        "        Args:\n",
        "            original_text (str): The original text.\n",
        "            summary (str): The summary to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary with evaluation metrics.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Try to import rouge\n",
        "            from rouge import Rouge\n",
        "            rouge = Rouge()\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            scores = rouge.get_scores(summary, original_text)\n",
        "\n",
        "            # Extract metrics\n",
        "            evaluation = {\n",
        "                \"rouge-1_precision\": scores[0][\"rouge-1\"][\"p\"],\n",
        "                \"rouge-1_recall\": scores[0][\"rouge-1\"][\"r\"],\n",
        "                \"rouge-1_f1\": scores[0][\"rouge-1\"][\"f\"],\n",
        "                \"rouge-2_f1\": scores[0][\"rouge-2\"][\"f\"],\n",
        "                \"rouge-l_f1\": scores[0][\"rouge-l\"][\"f\"]\n",
        "            }\n",
        "\n",
        "            # Overall score (weighted average of F1 scores)\n",
        "            evaluation[\"overall_score\"] = (\n",
        "                0.4 * evaluation[\"rouge-1_f1\"] +\n",
        "                0.3 * evaluation[\"rouge-2_f1\"] +\n",
        "                0.3 * evaluation[\"rouge-l_f1\"]\n",
        "            )\n",
        "\n",
        "            return evaluation\n",
        "\n",
        "        except ImportError:\n",
        "            # If rouge is not installed, return basic word overlap analysis\n",
        "            print(\"Rouge package not found. Using basic evaluation.\")\n",
        "            return self._basic_evaluation(original_text, summary)\n",
        "\n",
        "    def _basic_evaluation(self, original_text: str, summary: str) -> Dict[str, float]:\n",
        "        \"\"\"Simple evaluation when Rouge is not available.\"\"\"\n",
        "        orig_words = set(original_text.lower().split())\n",
        "        summary_words = set(summary.lower().split())\n",
        "\n",
        "        # Calculate word overlap\n",
        "        overlap = len(orig_words.intersection(summary_words))\n",
        "        precision = overlap / len(summary_words) if summary_words else 0\n",
        "        recall = overlap / len(orig_words) if orig_words else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "\n",
        "        return {\n",
        "            \"word_precision\": precision,\n",
        "            \"word_recall\": recall,\n",
        "            \"word_f1\": f1,\n",
        "            \"overall_score\": f1\n",
        "        }\n",
        "\n",
        "    def get_available_models(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Returns a list of recommended free models for summarization.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict[str, str]]: List of model information.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\n",
        "                \"name\": \"facebook/bart-large-cnn\",\n",
        "                \"description\": \"Good balance between quality and speed, trained on CNN/DM dataset\",\n",
        "                \"size\": \"400MB\",\n",
        "                \"strengths\": \"General purpose summarization, news articles\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"t5-small\",\n",
        "                \"description\": \"Smaller and faster model, good for resource-constrained environments\",\n",
        "                \"size\": \"242MB\",\n",
        "                \"strengths\": \"Speed, works on lower-end hardware\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"google/pegasus-xsum\",\n",
        "                \"description\": \"State-of-the-art model for extreme summarization (very concise)\",\n",
        "                \"size\": \"2.2GB\",\n",
        "                \"strengths\": \"Very concise summaries, news articles\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"philschmid/bart-large-cnn-samsum\",\n",
        "                \"description\": \"Fine-tuned for dialogue summarization\",\n",
        "                \"size\": \"400MB\",\n",
        "                \"strengths\": \"Conversations, chat logs, meetings\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"sshleifer/distilbart-cnn-12-6\",\n",
        "                \"description\": \"Distilled version of BART, faster with similar quality\",\n",
        "                \"size\": \"306MB\",\n",
        "                \"strengths\": \"Good balance of speed and quality\"\n",
        "            }\n",
        "        ]\n",
        "\n"
      ],
      "metadata": {
        "id": "Sf7JWzqbJAgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample text to summarize\n",
        "    sample_text = \"\"\"\n",
        "    An ancient jawbone dredged from the Taiwanese seabed has revealed new insights into the appearance and sweeping geographic range of an enigmatic human species called the Denisovans.\n",
        "\n",
        "    The fossil was discovered by fishers trawling the Penghu Channel off Taiwan and is thought to be the most complete fossil that has been genetically identified as Denisovan. The male individual, who lived at least 10,000 years ago, had a strong jaw and very large, powerful molars.\n",
        "\n",
        "    “From a tooth or a small bone fragment, there’s the mystery of their appearance,” said Prof Enrico Cappellini, of the University of Copenhagen, a co-senior author on the paper. A Denisovan jaw discovered in Tibet had begun to fill in this picture, and the latest discovery adds to the evidence of a prominent jaw with huge teeth.\n",
        "\n",
        "    “Now we have a richer image,” Cappellini said. “Of course it would be good to have a skull and the rest of the skeleton, but it’s a step forward.”\n",
        "\n",
        "    The fossil has been dated to one of two glacial periods when the channel is known to have been above sea level, either between 10,000 and 70,000 years ago or between 130,000 and 190,000 years ago.\n",
        "    The discovery reveals an impressive geographic range for the ancient species, which lived at the same time as – and interbred with – modern humans and Neanderthals.\n",
        "\n",
        "    The first Denisovan fossils, identified through analysis of ancient DNA, came from a cave in Siberia and comprised just a finger fragment and a few teeth. Since then, further discoveries show Denisovans also weathered the incredibly harsh conditions of the high-altitude Tibetan plateau, where temperatures can plunge to -30C. By contrast, in south-east Asia they would have lived alongside water buffaloes in a balmy climate.\n",
        "\n",
        "    “These are climate and environmental conditions that are quite different,” Cappellini said. “The cold environment in Siberia, high altitude in Tibet. We cannot infer anything of their cognitive abilities … but they had an ability to adapt to environments that are quite diverse.”\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Print available models\n",
        "        print(\"Available summarization models:\")\n",
        "        summarizer = TextSummarizer()  # Initialize with default model\n",
        "        models = summarizer.get_available_models()\n",
        "        for i, model in enumerate(models):\n",
        "            print(f\"{i+1}. {model['name']} - {model['description']} ({model['size']})\")\n",
        "\n",
        "        print(\"\\nUsing default model:\", summarizer.model_name)\n",
        "\n",
        "        # Generate a concise summary\n",
        "        print(\"\\nGenerating concise summary...\")\n",
        "        concise_summary = summarizer.summarize(\n",
        "            sample_text,\n",
        "            max_length=75,\n",
        "            min_length=30,\n",
        "            style=\"concise\"\n",
        "        )\n",
        "\n",
        "        # Generate a detailed summary\n",
        "        print(\"Generating detailed summary...\")\n",
        "        detailed_summary = summarizer.summarize(\n",
        "            sample_text,\n",
        "            style=\"detailed\"\n",
        "        )\n",
        "\n",
        "        # Generate bullet points\n",
        "        print(\"Generating bullet points...\")\n",
        "        bullet_points = summarizer.summarize(\n",
        "            sample_text,\n",
        "            style=\"bullet-points\"\n",
        "        )\n",
        "\n",
        "        # Evaluate the concise summary\n",
        "        print(\"Evaluating summary...\")\n",
        "        evaluation = summarizer.evaluate_summary(sample_text, concise_summary)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n--- ORIGINAL TEXT ---\")\n",
        "        print(textwrap.fill(sample_text, width=80))\n",
        "\n",
        "        print(\"\\n--- CONCISE SUMMARY ---\")\n",
        "        print(textwrap.fill(concise_summary, width=80))\n",
        "\n",
        "        print(\"\\n--- DETAILED SUMMARY ---\")\n",
        "        print(textwrap.fill(detailed_summary, width=80))\n",
        "\n",
        "        print(\"\\n--- BULLET POINTS ---\")\n",
        "        print(bullet_points)\n",
        "\n",
        "        print(\"\\n--- EVALUATION ---\")\n",
        "        for metric, score in evaluation.items():\n",
        "            print(f\"{metric}: {score:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"1. Make sure you have installed the required packages:\")\n",
        "        print(\"   pip install torch transformers\")\n",
        "        print(\"2. For evaluation metrics, install rouge:\")\n",
        "        print(\"   pip install rouge\")\n",
        "        print(\"3. If you have limited RAM, try using a smaller model:\")\n",
        "        print(\"   summarizer = TextSummarizer('t5-small')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cl78icVJEFn",
        "outputId": "4d165020-7ad2-400e-994a-32ddbd4b2276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available summarization models:\n",
            "Loading model: facebook/bart-large-cnn (this may take a minute on first run)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully. Using device: cpu\n",
            "1. facebook/bart-large-cnn - Good balance between quality and speed, trained on CNN/DM dataset (400MB)\n",
            "2. t5-small - Smaller and faster model, good for resource-constrained environments (242MB)\n",
            "3. google/pegasus-xsum - State-of-the-art model for extreme summarization (very concise) (2.2GB)\n",
            "4. philschmid/bart-large-cnn-samsum - Fine-tuned for dialogue summarization (400MB)\n",
            "5. sshleifer/distilbart-cnn-12-6 - Distilled version of BART, faster with similar quality (306MB)\n",
            "\n",
            "Using default model: facebook/bart-large-cnn\n",
            "\n",
            "Generating concise summary...\n",
            "Generating detailed summary...\n",
            "Generating bullet points...\n",
            "Evaluating summary...\n",
            "Rouge package not found. Using basic evaluation.\n",
            "\n",
            "--- ORIGINAL TEXT ---\n",
            "     An ancient jawbone dredged from the Taiwanese seabed has revealed new\n",
            "insights into the appearance and sweeping geographic range of an enigmatic human\n",
            "species called the Denisovans.      The fossil was discovered by fishers\n",
            "trawling the Penghu Channel off Taiwan and is thought to be the most complete\n",
            "fossil that has been genetically identified as Denisovan. The male individual,\n",
            "who lived at least 10,000 years ago, had a strong jaw and very large, powerful\n",
            "molars.      “From a tooth or a small bone fragment, there’s the mystery of\n",
            "their appearance,” said Prof Enrico Cappellini, of the University of Copenhagen,\n",
            "a co-senior author on the paper. A Denisovan jaw discovered in Tibet had begun\n",
            "to fill in this picture, and the latest discovery adds to the evidence of a\n",
            "prominent jaw with huge teeth.      “Now we have a richer image,” Cappellini\n",
            "said. “Of course it would be good to have a skull and the rest of the skeleton,\n",
            "but it’s a step forward.”      The fossil has been dated to one of two glacial\n",
            "periods when the channel is known to have been above sea level, either between\n",
            "10,000 and 70,000 years ago or between 130,000 and 190,000 years ago.     The\n",
            "discovery reveals an impressive geographic range for the ancient species, which\n",
            "lived at the same time as – and interbred with – modern humans and Neanderthals.\n",
            "The first Denisovan fossils, identified through analysis of ancient DNA, came\n",
            "from a cave in Siberia and comprised just a finger fragment and a few teeth.\n",
            "Since then, further discoveries show Denisovans also weathered the incredibly\n",
            "harsh conditions of the high-altitude Tibetan plateau, where temperatures can\n",
            "plunge to -30C. By contrast, in south-east Asia they would have lived alongside\n",
            "water buffaloes in a balmy climate.      “These are climate and environmental\n",
            "conditions that are quite different,” Cappellini said. “The cold environment in\n",
            "Siberia, high altitude in Tibet. We cannot infer anything of their cognitive\n",
            "abilities … but they had an ability to adapt to environments that are quite\n",
            "diverse.”\n",
            "\n",
            "--- CONCISE SUMMARY ---\n",
            "The fossil was discovered by fishers trawling the Penghu Channel off Taiwan. It\n",
            "is thought to be the most complete fossil that has been genetically identified\n",
            "as Denisovan. The male individual had a strong jaw and very large, powerful\n",
            "molars.\n",
            "\n",
            "--- DETAILED SUMMARY ---\n",
            "The fossil was discovered by fishers trawling the Penghu Channel off Taiwan. It\n",
            "is thought to be the most complete fossil that has been genetically identified\n",
            "as Denisovan. The male individual, who lived at least 10,000 years ago, had a\n",
            "strong jaw and very large, powerful molars. The first Denisovan fossils,\n",
            "identified through analysis of ancient DNA, came from a cave in Siberia and\n",
            "comprised just a finger fragment and a few teeth. Since then, further\n",
            "discoveries show Denisovans also weathered the incredibly harsh conditions of\n",
            "the high-altitude Tibetan plateau.\n",
            "\n",
            "--- BULLET POINTS ---\n",
            "• The fossil was discovered by fishers trawling the Penghu Channel off Taiwan.\n",
            "• It is thought to be the most complete fossil that has been genetically identified as Denisovan.\n",
            "• The male individual had a strong jaw and very large, powerful molars.\n",
            "\n",
            "--- EVALUATION ---\n",
            "word_precision: 0.944\n",
            "word_recall: 0.162\n",
            "word_f1: 0.276\n",
            "overall_score: 0.276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarize pdf/word doc"
      ],
      "metadata": {
        "id": "ADlT9_ezYz9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xKaa-51Y3yh",
        "outputId": "23f6bb39-0137-462f-b8de-1b676f8b3ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKcO2gmuZF6c",
        "outputId": "b019e967-864f-4627-942a-d3780032d4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.1)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from typing import List, Dict, Any, Optional, Union, IO\n",
        "import textwrap\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "LpuTbaYDZJfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentTextExtractor:\n",
        "    \"\"\"Extract text from various document formats like PDF and Word.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the document text extractor.\"\"\"\n",
        "        # Check for required libraries\n",
        "        self._check_dependencies()\n",
        "\n",
        "    def _check_dependencies(self):\n",
        "        \"\"\"Check if required libraries are installed and provide installation instructions if not.\"\"\"\n",
        "        missing_libs = []\n",
        "\n",
        "        # Check for PDF support\n",
        "        try:\n",
        "            import PyPDF2\n",
        "        except ImportError:\n",
        "            missing_libs.append(\"PyPDF2\")\n",
        "\n",
        "        # Check for Word document support\n",
        "        try:\n",
        "            import docx\n",
        "        except ImportError:\n",
        "            missing_libs.append(\"python-docx\")\n",
        "\n",
        "        # Print installation instructions if libraries are missing\n",
        "        if missing_libs:\n",
        "            print(\"Missing required libraries for document processing:\")\n",
        "            print(f\"pip install {' '.join(missing_libs)}\")\n",
        "            print(\"Installing these libraries will enable PDF and Word document support.\")\n",
        "\n",
        "    def extract_text(self, document_path: Union[str, Path, IO]) -> str:\n",
        "        \"\"\"\n",
        "        Extract text from a document file (PDF or Word).\n",
        "\n",
        "        Args:\n",
        "            document_path: Path to the document or file-like object\n",
        "\n",
        "        Returns:\n",
        "            str: Extracted text content\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the document format is unsupported\n",
        "            ImportError: If required libraries are not installed\n",
        "        \"\"\"\n",
        "        # Handle file path as string or Path object\n",
        "        if isinstance(document_path, (str, Path)):\n",
        "            file_path = str(document_path)\n",
        "            file_ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "            # Extract text based on file extension\n",
        "            if file_ext == '.pdf':\n",
        "                return self._extract_from_pdf(file_path)\n",
        "            elif file_ext in ['.docx', '.doc']:\n",
        "                return self._extract_from_word(file_path)\n",
        "            else:\n",
        "                try:\n",
        "                    # Try to read as plain text\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        return f.read()\n",
        "                except Exception:\n",
        "                    raise ValueError(f\"Unsupported document format: {file_ext}\")\n",
        "\n",
        "        # Handle file-like objects\n",
        "        else:\n",
        "            # Create a temporary file to save the content\n",
        "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                temp_path = temp_file.name\n",
        "                # Read and write content\n",
        "                document_path.seek(0)\n",
        "                temp_file.write(document_path.read())\n",
        "\n",
        "            try:\n",
        "                # Try different extractors\n",
        "                try:\n",
        "                    return self._extract_from_pdf(temp_path)\n",
        "                except Exception:\n",
        "                    try:\n",
        "                        return self._extract_from_word(temp_path)\n",
        "                    except Exception:\n",
        "                        # Try as plain text\n",
        "                        with open(temp_path, 'r', encoding='utf-8') as f:\n",
        "                            return f.read()\n",
        "            finally:\n",
        "                # Clean up temp file\n",
        "                if os.path.exists(temp_path):\n",
        "                    os.unlink(temp_path)\n",
        "\n",
        "    def _extract_from_pdf(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from a PDF file.\"\"\"\n",
        "        try:\n",
        "            import PyPDF2\n",
        "        except ImportError:\n",
        "            raise ImportError(\"PyPDF2 is required for PDF support. Install with: pip install PyPDF2\")\n",
        "\n",
        "        text = \"\"\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text() + \"\\n\\n\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _extract_from_word(self, docx_path: str) -> str:\n",
        "        \"\"\"Extract text from a Word document.\"\"\"\n",
        "        try:\n",
        "            import docx\n",
        "        except ImportError:\n",
        "            raise ImportError(\"python-docx is required for Word document support. Install with: pip install python-docx\")\n",
        "\n",
        "        doc = docx.Document(docx_path)\n",
        "        text = []\n",
        "\n",
        "        for para in doc.paragraphs:\n",
        "            text.append(para.text)\n",
        "\n",
        "        return '\\n'.join(text)\n"
      ],
      "metadata": {
        "id": "HUR6HHAFZWqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextSummarizer:\n",
        "    \"\"\"A text summarization model using free, locally-run models with document support.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\"):\n",
        "        \"\"\"\n",
        "        Initialize the text summarizer with a Hugging Face model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the Hugging Face model to use.\n",
        "                Default: \"facebook/bart-large-cnn\" (good balance of quality and speed)\n",
        "                Other options:\n",
        "                - \"t5-small\" (faster, lower quality)\n",
        "                - \"google/pegasus-xsum\" (news-focused)\n",
        "                - \"philschmid/bart-large-cnn-samsum\" (optimized for dialogues)\n",
        "        \"\"\"\n",
        "        print(f\"Loading model: {model_name} (this may take a minute on first run)\")\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "            # For advanced summarization\n",
        "            self.summarization_pipeline = pipeline(\n",
        "                \"summarization\",\n",
        "                model=model_name,\n",
        "                device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "            )\n",
        "\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            print(f\"Model loaded successfully. Using device: {self.device}\")\n",
        "\n",
        "            # Store model name for reference\n",
        "            self.model_name = model_name\n",
        "\n",
        "            # Initialize document extractor\n",
        "            self.document_extractor = DocumentTextExtractor()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            print(\"Trying to load a smaller model as fallback...\")\n",
        "            try:\n",
        "                # Fallback to a smaller model\n",
        "                fallback_model = \"t5-small\"\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n",
        "                self.model = AutoModelForSeq2SeqLM.from_pretrained(fallback_model)\n",
        "                self.summarization_pipeline = pipeline(\n",
        "                    \"summarization\",\n",
        "                    model=fallback_model,\n",
        "                    device=0 if torch.cuda.is_available() else -1\n",
        "                )\n",
        "                self.model_name = fallback_model\n",
        "                print(f\"Fallback model loaded: {fallback_model}\")\n",
        "\n",
        "                # Initialize document extractor\n",
        "                self.document_extractor = DocumentTextExtractor()\n",
        "            except Exception as e2:\n",
        "                raise RuntimeError(f\"Failed to load both primary and fallback models: {e2}\")\n",
        "\n",
        "    def summarize_document(self, document_path: Union[str, Path, IO], **kwargs) -> str:\n",
        "        \"\"\"\n",
        "        Summarize a document file (PDF, Word, or text).\n",
        "\n",
        "        Args:\n",
        "            document_path: Path to the document or file-like object\n",
        "            **kwargs: Additional arguments passed to summarize method\n",
        "\n",
        "        Returns:\n",
        "            str: The generated summary\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Extract text from document\n",
        "            print(\"Extracting text from document...\")\n",
        "            text = self.document_extractor.extract_text(document_path)\n",
        "\n",
        "            # Summarize the extracted text\n",
        "            print(f\"Summarizing document ({len(text.split())} words)...\")\n",
        "            return self.summarize(text, **kwargs)\n",
        "        except ImportError as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return \"Required libraries not installed. See error message above for installation instructions.\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document: {e}\")\n",
        "            return f\"Failed to summarize document: {str(e)}\"\n",
        "\n",
        "    def summarize(self, text: str, max_length: int = 150, min_length: int = 30,\n",
        "                  style: str = \"concise\", preserve_key_points: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Summarize the given text using the loaded model.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to summarize.\n",
        "            max_length (int): Maximum token length for the summary.\n",
        "            min_length (int): Minimum token length for the summary.\n",
        "            style (str): Style of summary (\"concise\", \"detailed\", \"bullet-points\").\n",
        "            preserve_key_points (bool): Whether to try to preserve key information.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated summary.\n",
        "        \"\"\"\n",
        "        # Adjust parameters based on style\n",
        "        if style == \"concise\":\n",
        "            # Keep defaults, shorter summary\n",
        "            pass\n",
        "        elif style == \"detailed\":\n",
        "            # Increase length for more details\n",
        "            max_length = max(max_length * 2, 250)\n",
        "            min_length = max(min_length * 2, 100)\n",
        "        elif style == \"bullet-points\":\n",
        "            # Will be post-processed into bullet points\n",
        "            pass\n",
        "\n",
        "        # For more control, directly set parameters for pipeline\n",
        "        num_beams = 4\n",
        "        if preserve_key_points:\n",
        "            # Using more beams and lower temperature helps preserve key information\n",
        "            num_beams = 6\n",
        "            temperature = 0.7\n",
        "        else:\n",
        "            temperature = 1.0\n",
        "\n",
        "        # Handle long texts by chunking if needed\n",
        "        max_input_length = self.tokenizer.model_max_length\n",
        "        input_ids = self.tokenizer.encode(text, truncation=False)\n",
        "\n",
        "        if len(input_ids) > max_input_length:\n",
        "            # Text is too long, need to chunk it\n",
        "            return self._summarize_long_text(text, max_length, min_length, num_beams, temperature)\n",
        "\n",
        "        # Generate summary\n",
        "        summary = self.summarization_pipeline(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        result = summary[0]['summary_text']\n",
        "\n",
        "        # Post-process for bullet points if requested\n",
        "        if style == \"bullet-points\":\n",
        "            # Convert to bullet points by identifying key sentences\n",
        "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
        "            bullet_points = \"\\n\".join([f\"• {s}.\" for s in sentences])\n",
        "            return bullet_points\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _summarize_long_text(self, text: str, max_length: int, min_length: int,\n",
        "                             num_beams: int, temperature: float) -> str:\n",
        "        \"\"\"\n",
        "        Handle summarization of texts longer than the model's maximum input length.\n",
        "        Uses a chunking approach to process long documents.\n",
        "\n",
        "        Args:\n",
        "            text (str): The long text to summarize.\n",
        "            max_length (int): Maximum token length for the final summary.\n",
        "            min_length (int): Minimum token length for the final summary.\n",
        "            num_beams (int): Number of beams for beam search.\n",
        "            temperature (float): Temperature for generation.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated summary.\n",
        "        \"\"\"\n",
        "        # Split text into sentences\n",
        "        sentences = [s.strip() + '.' for s in text.replace('\\n', ' ').split('.') if s.strip()]\n",
        "\n",
        "        # Calculate max tokens for model\n",
        "        max_tokens = self.tokenizer.model_max_length - 50  # Reserve tokens for generation\n",
        "\n",
        "        # Create chunks that fit within model's context window\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = len(self.tokenizer.encode(sentence))\n",
        "\n",
        "            if current_length + sentence_tokens > max_tokens:\n",
        "                # This chunk is full, start a new one\n",
        "                if current_chunk:  # Make sure we don't add empty chunks\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_tokens\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += sentence_tokens\n",
        "\n",
        "        # Add the last chunk if it's not empty\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        # Summarize each chunk\n",
        "        chunk_summaries = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "            chunk_summary = self.summarization_pipeline(\n",
        "                chunk,\n",
        "                max_length=max(30, max_length // len(chunks)),  # Scale based on chunks\n",
        "                min_length=min(min_length // len(chunks), 20),  # Ensure some minimum\n",
        "                num_beams=num_beams,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            chunk_summaries.append(chunk_summary[0]['summary_text'])\n",
        "\n",
        "        # If we have multiple chunk summaries, summarize them together\n",
        "        if len(chunk_summaries) > 1:\n",
        "            combined_summary = \" \".join(chunk_summaries)\n",
        "\n",
        "            # Check if the combined summary is still too long\n",
        "            if len(self.tokenizer.encode(combined_summary)) > max_tokens:\n",
        "                # Recursively summarize the combined summaries\n",
        "                return self._summarize_long_text(\n",
        "                    combined_summary, max_length, min_length, num_beams, temperature\n",
        "                )\n",
        "            else:\n",
        "                # Final summary of the combined chunk summaries\n",
        "                final_summary = self.summarization_pipeline(\n",
        "                    combined_summary,\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    num_beams=num_beams,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                return final_summary[0]['summary_text']\n",
        "        else:\n",
        "            # If we only had one chunk, its summary is the final summary\n",
        "            return chunk_summaries[0]\n",
        "\n",
        "    def batch_summarize(self, texts: List[str], **kwargs) -> List[str]:\n",
        "        \"\"\"\n",
        "        Summarize multiple texts at once.\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): List of texts to summarize.\n",
        "            **kwargs: Additional arguments passed to summarize method.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of summaries.\n",
        "        \"\"\"\n",
        "        return [self.summarize(text, **kwargs) for text in texts]\n",
        "\n",
        "    def batch_summarize_documents(self, document_paths: List[Union[str, Path]], **kwargs) -> List[str]:\n",
        "        \"\"\"\n",
        "        Summarize multiple documents at once.\n",
        "\n",
        "        Args:\n",
        "            document_paths: List of paths to documents\n",
        "            **kwargs: Additional arguments passed to summarize method\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of summaries\n",
        "        \"\"\"\n",
        "        return [self.summarize_document(doc_path, **kwargs) for doc_path in document_paths]\n",
        "\n",
        "    def evaluate_summary(self, original_text: str, summary: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the quality of a summary compared to the original text using ROUGE metrics.\n",
        "\n",
        "        Args:\n",
        "            original_text (str): The original text.\n",
        "            summary (str): The summary to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary with evaluation metrics.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Try to import rouge\n",
        "            from rouge import Rouge\n",
        "            rouge = Rouge()\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            scores = rouge.get_scores(summary, original_text)\n",
        "\n",
        "            # Extract metrics\n",
        "            evaluation = {\n",
        "                \"rouge-1_precision\": scores[0][\"rouge-1\"][\"p\"],\n",
        "                \"rouge-1_recall\": scores[0][\"rouge-1\"][\"r\"],\n",
        "                \"rouge-1_f1\": scores[0][\"rouge-1\"][\"f\"],\n",
        "                \"rouge-2_f1\": scores[0][\"rouge-2\"][\"f\"],\n",
        "                \"rouge-l_f1\": scores[0][\"rouge-l\"][\"f\"]\n",
        "            }\n",
        "\n",
        "            # Overall score (weighted average of F1 scores)\n",
        "            evaluation[\"overall_score\"] = (\n",
        "                0.4 * evaluation[\"rouge-1_f1\"] +\n",
        "                0.3 * evaluation[\"rouge-2_f1\"] +\n",
        "                0.3 * evaluation[\"rouge-l_f1\"]\n",
        "            )\n",
        "\n",
        "            return evaluation\n",
        "\n",
        "        except ImportError:\n",
        "            # If rouge is not installed, return basic word overlap analysis\n",
        "            print(\"Rouge package not found. Using basic evaluation.\")\n",
        "            return self._basic_evaluation(original_text, summary)\n",
        "\n",
        "    def _basic_evaluation(self, original_text: str, summary: str) -> Dict[str, float]:\n",
        "        \"\"\"Simple evaluation when Rouge is not available.\"\"\"\n",
        "        orig_words = set(original_text.lower().split())\n",
        "        summary_words = set(summary.lower().split())\n",
        "\n",
        "        # Calculate word overlap\n",
        "        overlap = len(orig_words.intersection(summary_words))\n",
        "        precision = overlap / len(summary_words) if summary_words else 0\n",
        "        recall = overlap / len(orig_words) if orig_words else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "\n",
        "        return {\n",
        "            \"word_precision\": precision,\n",
        "            \"word_recall\": recall,\n",
        "            \"word_f1\": f1,\n",
        "            \"overall_score\": f1\n",
        "        }\n",
        "\n",
        "    def get_available_models(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Returns a list of recommended free models for summarization.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict[str, str]]: List of model information.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\n",
        "                \"name\": \"facebook/bart-large-cnn\",\n",
        "                \"description\": \"Good balance between quality and speed, trained on CNN/DM dataset\",\n",
        "                \"size\": \"400MB\",\n",
        "                \"strengths\": \"General purpose summarization, news articles\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"t5-small\",\n",
        "                \"description\": \"Smaller and faster model, good for resource-constrained environments\",\n",
        "                \"size\": \"242MB\",\n",
        "                \"strengths\": \"Speed, works on lower-end hardware\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"google/pegasus-xsum\",\n",
        "                \"description\": \"State-of-the-art model for extreme summarization (very concise)\",\n",
        "                \"size\": \"2.2GB\",\n",
        "                \"strengths\": \"Very concise summaries, news articles\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"philschmid/bart-large-cnn-samsum\",\n",
        "                \"description\": \"Fine-tuned for dialogue summarization\",\n",
        "                \"size\": \"400MB\",\n",
        "                \"strengths\": \"Conversations, chat logs, meetings\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"sshleifer/distilbart-cnn-12-6\",\n",
        "                \"description\": \"Distilled version of BART, faster with similar quality\",\n",
        "                \"size\": \"306MB\",\n",
        "                \"strengths\": \"Good balance of speed and quality\"\n",
        "            }\n",
        "        ]\n",
        "\n"
      ],
      "metadata": {
        "id": "PB-zJw4fZi5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcYtr9_8a-Ce",
        "outputId": "6b789bce-675f-4388-d86a-f28acc007553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize the summarizer\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "    # Example: Summarize a PDF document\n",
        "    # (Uncomment and modify with your PDF path)\n",
        "\n",
        "    docx_path = \"/content/drive/My Drive/SumText_1.docx\"\n",
        "    if os.path.exists(docx_path):\n",
        "        print(f\"\\nSummarizing Word document: {docx_path}\")\n",
        "        docx_summary = summarizer.summarize_document(\n",
        "            docx_path,\n",
        "            style=\"detailed\"\n",
        "        )\n",
        "        print(\"\\n--- WORD DOCUMENT SUMMARY ---\")\n",
        "        print(docx_summary)\n",
        "    else:\n",
        "        print(f\"Word document not found: {docx_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynjaCOrCaDjT",
        "outputId": "850ae45d-901c-4c0e-ab9e-8d74f35e3b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: facebook/bart-large-cnn (this may take a minute on first run)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully. Using device: cpu\n",
            "\n",
            "Summarizing Word document: /content/drive/My Drive/SumText_1.docx\n",
            "Extracting text from document...\n",
            "Summarizing document (678 words)...\n",
            "\n",
            "--- WORD DOCUMENT SUMMARY ---\n",
            "US and Ukrainian officials met on Friday to discuss White House proposals for a minerals deal. Donald Trump wants Kyiv to hand over its natural resources as ‘payback’ in return for weapons delivered by the previous Biden administration. Latest US draft is more ‘maximalist’ than the original version from February, which proposed giving Washington $500bn worth of rare metals, as well as oil and gas. Ukraine cut off the supply of gas when its five-year contract with the Russian state energy company Gazprom expired.\n"
          ]
        }
      ]
    }
  ]
}