{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fTkwHkR-6cGi",
        "G0N6xxOs6o9m",
        "Yokdwcp66vqC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#BART-Large-CNN Text Summarization\n"
      ],
      "metadata": {
        "id": "wruJeSc3OzpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Libraries and Dependencies"
      ],
      "metadata": {
        "id": "fTkwHkR-6cGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6WaRMHODYpx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from typing import List, Dict, Any, Optional\n",
        "import textwrap\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation"
      ],
      "metadata": {
        "id": "G0N6xxOs6o9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\"):\n",
        "        \"\"\"\n",
        "        Initialize the text summarizer with a Hugging Face model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the Hugging Face model to use.\n",
        "                Default: \"facebook/bart-large-cnn\" (good balance of quality and speed)\n",
        "                Other options:\n",
        "                - \"t5-small\" (faster, lower quality)\n",
        "                - \"google/pegasus-xsum\" (news-focused)\n",
        "                - \"philschmid/bart-large-cnn-samsum\" (optimized for dialogues)\n",
        "        \"\"\"\n",
        "        print(f\"Loading model: {model_name} (this may take a minute on first run)\")\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "            # For advanced summarization\n",
        "            self.summarization_pipeline = pipeline(\n",
        "                \"summarization\",\n",
        "                model=model_name,\n",
        "                device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
        "            )\n",
        "\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            print(f\"Model loaded successfully. Using device: {self.device}\")\n",
        "\n",
        "            # Store model name for reference\n",
        "            self.model_name = model_name\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            print(\"Trying to load a smaller model as fallback...\")\n",
        "            try:\n",
        "                # Fallback to a smaller model\n",
        "                fallback_model = \"t5-small\"\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)\n",
        "                self.model = AutoModelForSeq2SeqLM.from_pretrained(fallback_model)\n",
        "                self.summarization_pipeline = pipeline(\n",
        "                    \"summarization\",\n",
        "                    model=fallback_model,\n",
        "                    device=0 if torch.cuda.is_available() else -1\n",
        "                )\n",
        "                self.model_name = fallback_model\n",
        "                print(f\"Fallback model loaded: {fallback_model}\")\n",
        "            except Exception as e2:\n",
        "                raise RuntimeError(f\"Failed to load both primary and fallback models: {e2}\")\n",
        "\n",
        "    def summarize(self, text: str, max_length: int = 150, min_length: int = 30,\n",
        "                  style: str = \"concise\", preserve_key_points: bool = True) -> str:\n",
        "        \"\"\"\n",
        "        Summarize the given text using the loaded model.\n",
        "\n",
        "        Args:\n",
        "            text (str): The text to summarize.\n",
        "            max_length (int): Maximum token length for the summary.\n",
        "            min_length (int): Minimum token length for the summary.\n",
        "            style (str): Style of summary (\"concise\", \"detailed\", \"bullet-points\").\n",
        "            preserve_key_points (bool): Whether to try to preserve key information.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated summary.\n",
        "        \"\"\"\n",
        "        # Adjust parameters based on style\n",
        "        if style == \"concise\":\n",
        "            # Keep defaults, shorter summary\n",
        "            pass\n",
        "        elif style == \"detailed\":\n",
        "            # Increase length for more details\n",
        "            max_length = max(max_length * 2, 250)\n",
        "            min_length = max(min_length * 2, 100)\n",
        "        elif style == \"bullet-points\":\n",
        "            # Will be post-processed into bullet points\n",
        "            pass\n",
        "\n",
        "        # For more control, directly set parameters for pipeline\n",
        "        num_beams = 4\n",
        "        if preserve_key_points:\n",
        "            # Using more beams and lower temperature helps preserve key information\n",
        "            num_beams = 6\n",
        "            temperature = 0.7\n",
        "        else:\n",
        "            temperature = 1.0\n",
        "\n",
        "        # Handle long texts by chunking if needed\n",
        "        max_input_length = self.tokenizer.model_max_length\n",
        "        input_ids = self.tokenizer.encode(text, truncation=False)\n",
        "\n",
        "        if len(input_ids) > max_input_length:\n",
        "            # Text is too long, need to chunk it\n",
        "            return self._summarize_long_text(text, max_length, min_length, num_beams, temperature)\n",
        "\n",
        "        # Generate summary\n",
        "        summary = self.summarization_pipeline(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            min_length=min_length,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        result = summary[0]['summary_text']\n",
        "\n",
        "        # Post-process for bullet points if requested\n",
        "        if style == \"bullet-points\":\n",
        "            # Convert to bullet points by identifying key sentences\n",
        "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
        "            bullet_points = \"\\n\".join([f\"• {s}.\" for s in sentences])\n",
        "            return bullet_points\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _summarize_long_text(self, text: str, max_length: int, min_length: int,\n",
        "                             num_beams: int, temperature: float) -> str:\n",
        "        \"\"\"\n",
        "        Handle summarization of texts longer than the model's maximum input length.\n",
        "        Uses a chunking approach to process long documents.\n",
        "\n",
        "        Args:\n",
        "            text (str): The long text to summarize.\n",
        "            max_length (int): Maximum token length for the final summary.\n",
        "            min_length (int): Minimum token length for the final summary.\n",
        "            num_beams (int): Number of beams for beam search.\n",
        "            temperature (float): Temperature for generation.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated summary.\n",
        "        \"\"\"\n",
        "        # Split text into sentences\n",
        "        sentences = [s.strip() + '.' for s in text.replace('\\n', ' ').split('.') if s.strip()]\n",
        "\n",
        "        # Calculate max tokens for model\n",
        "        max_tokens = self.tokenizer.model_max_length - 50  # Reserve tokens for generation\n",
        "\n",
        "        # Create chunks that fit within model's context window\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = len(self.tokenizer.encode(sentence))\n",
        "\n",
        "            if current_length + sentence_tokens > max_tokens:\n",
        "                # This chunk is full, start a new one\n",
        "                if current_chunk:  # Make sure we don't add empty chunks\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_tokens\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += sentence_tokens\n",
        "\n",
        "        # Add the last chunk if it's not empty\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        # Summarize each chunk\n",
        "        chunk_summaries = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
        "            chunk_summary = self.summarization_pipeline(\n",
        "                chunk,\n",
        "                max_length=max(30, max_length // len(chunks)),  # Scale based on chunks\n",
        "                min_length=min(min_length // len(chunks), 20),  # Ensure some minimum\n",
        "                num_beams=num_beams,\n",
        "                temperature=temperature\n",
        "            )\n",
        "            chunk_summaries.append(chunk_summary[0]['summary_text'])\n",
        "\n",
        "        # If we have multiple chunk summaries, summarize them together\n",
        "        if len(chunk_summaries) > 1:\n",
        "            combined_summary = \" \".join(chunk_summaries)\n",
        "\n",
        "            # Check if the combined summary is still too long\n",
        "            if len(self.tokenizer.encode(combined_summary)) > max_tokens:\n",
        "                # Recursively summarize the combined summaries\n",
        "                return self._summarize_long_text(\n",
        "                    combined_summary, max_length, min_length, num_beams, temperature\n",
        "                )\n",
        "            else:\n",
        "                # Final summary of the combined chunk summaries\n",
        "                final_summary = self.summarization_pipeline(\n",
        "                    combined_summary,\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    num_beams=num_beams,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                return final_summary[0]['summary_text']\n",
        "        else:\n",
        "            # If we only had one chunk, its summary is the final summary\n",
        "            return chunk_summaries[0]\n",
        "\n",
        "    def batch_summarize(self, texts: List[str], **kwargs) -> List[str]:\n",
        "        \"\"\"\n",
        "        Summarize multiple texts at once.\n",
        "\n",
        "        Args:\n",
        "            texts (List[str]): List of texts to summarize.\n",
        "            **kwargs: Additional arguments passed to summarize method.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: List of summaries.\n",
        "        \"\"\"\n",
        "        return [self.summarize(text, **kwargs) for text in texts]\n",
        "\n",
        "    def evaluate_summary(self, original_text: str, summary: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the quality of a summary compared to the original text using ROUGE metrics.\n",
        "\n",
        "        Args:\n",
        "            original_text (str): The original text.\n",
        "            summary (str): The summary to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary with evaluation metrics.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Try to import rouge\n",
        "            from rouge import Rouge\n",
        "            rouge = Rouge()\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            scores = rouge.get_scores(summary, original_text)\n",
        "\n",
        "            # Extract metrics\n",
        "            evaluation = {\n",
        "                \"rouge-1_precision\": scores[0][\"rouge-1\"][\"p\"],\n",
        "                \"rouge-1_recall\": scores[0][\"rouge-1\"][\"r\"],\n",
        "                \"rouge-1_f1\": scores[0][\"rouge-1\"][\"f\"],\n",
        "                \"rouge-2_f1\": scores[0][\"rouge-2\"][\"f\"],\n",
        "                \"rouge-l_f1\": scores[0][\"rouge-l\"][\"f\"]\n",
        "            }\n",
        "\n",
        "            # Overall score (weighted average of F1 scores)\n",
        "            evaluation[\"overall_score\"] = (\n",
        "                0.4 * evaluation[\"rouge-1_f1\"] +\n",
        "                0.3 * evaluation[\"rouge-2_f1\"] +\n",
        "                0.3 * evaluation[\"rouge-l_f1\"]\n",
        "            )\n",
        "\n",
        "            return evaluation\n",
        "\n",
        "        except ImportError:\n",
        "            # If rouge is not installed, return basic word overlap analysis\n",
        "            print(\"Rouge package not found. Using basic evaluation.\")\n",
        "            return self._basic_evaluation(original_text, summary)\n",
        "\n",
        "    def _basic_evaluation(self, original_text: str, summary: str) -> Dict[str, float]:\n",
        "        \"\"\"Simple evaluation when Rouge is not available.\"\"\"\n",
        "        orig_words = set(original_text.lower().split())\n",
        "        summary_words = set(summary.lower().split())\n",
        "\n",
        "        # Calculate word overlap\n",
        "        overlap = len(orig_words.intersection(summary_words))\n",
        "        precision = overlap / len(summary_words) if summary_words else 0\n",
        "        recall = overlap / len(orig_words) if orig_words else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "\n",
        "        return {\n",
        "            \"word_precision\": precision,\n",
        "            \"word_recall\": recall,\n",
        "            \"word_f1\": f1,\n",
        "            \"overall_score\": f1\n",
        "        }\n",
        "\n",
        "    def get_available_models(self) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Returns a list of recommended free models for summarization.\n",
        "\n",
        "        Returns:\n",
        "            List[Dict[str, str]]: List of model information.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\n",
        "                \"name\": \"facebook/bart-large-cnn\",\n",
        "                \"description\": \"Good balance between quality and speed, trained on CNN/DM dataset\",\n",
        "                \"size\": \"400MB\",\n",
        "                \"strengths\": \"General purpose summarization, news articles\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"t5-small\",\n",
        "                \"description\": \"Smaller and faster model, good for resource-constrained environments\",\n",
        "                \"size\": \"242MB\",\n",
        "                \"strengths\": \"Speed, works on lower-end hardware\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"google/pegasus-xsum\",\n",
        "                \"description\": \"State-of-the-art model for extreme summarization (very concise)\",\n",
        "                \"size\": \"2.2GB\",\n",
        "                \"strengths\": \"Very concise summaries, news articles\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"philschmid/bart-large-cnn-samsum\",\n",
        "                \"description\": \"Fine-tuned for dialogue summarization\",\n",
        "                \"size\": \"400MB\",\n",
        "                \"strengths\": \"Conversations, chat logs, meetings\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"sshleifer/distilbart-cnn-12-6\",\n",
        "                \"description\": \"Distilled version of BART, faster with similar quality\",\n",
        "                \"size\": \"306MB\",\n",
        "                \"strengths\": \"Good balance of speed and quality\"\n",
        "            }\n",
        "        ]\n",
        "\n"
      ],
      "metadata": {
        "id": "Sf7JWzqbJAgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main function"
      ],
      "metadata": {
        "id": "Yokdwcp66vqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample text to summarize\n",
        "    sample_text = \"\"\"\n",
        "    An ancient jawbone dredged from the Taiwanese seabed has revealed new insights into the appearance and sweeping geographic range of an enigmatic human species called the Denisovans.\n",
        "\n",
        "    The fossil was discovered by fishers trawling the Penghu Channel off Taiwan and is thought to be the most complete fossil that has been genetically identified as Denisovan. The male individual, who lived at least 10,000 years ago, had a strong jaw and very large, powerful molars.\n",
        "\n",
        "    “From a tooth or a small bone fragment, there’s the mystery of their appearance,” said Prof Enrico Cappellini, of the University of Copenhagen, a co-senior author on the paper. A Denisovan jaw discovered in Tibet had begun to fill in this picture, and the latest discovery adds to the evidence of a prominent jaw with huge teeth.\n",
        "\n",
        "    “Now we have a richer image,” Cappellini said. “Of course it would be good to have a skull and the rest of the skeleton, but it’s a step forward.”\n",
        "\n",
        "    The fossil has been dated to one of two glacial periods when the channel is known to have been above sea level, either between 10,000 and 70,000 years ago or between 130,000 and 190,000 years ago.\n",
        "    The discovery reveals an impressive geographic range for the ancient species, which lived at the same time as – and interbred with – modern humans and Neanderthals.\n",
        "\n",
        "    The first Denisovan fossils, identified through analysis of ancient DNA, came from a cave in Siberia and comprised just a finger fragment and a few teeth. Since then, further discoveries show Denisovans also weathered the incredibly harsh conditions of the high-altitude Tibetan plateau, where temperatures can plunge to -30C. By contrast, in south-east Asia they would have lived alongside water buffaloes in a balmy climate.\n",
        "\n",
        "    “These are climate and environmental conditions that are quite different,” Cappellini said. “The cold environment in Siberia, high altitude in Tibet. We cannot infer anything of their cognitive abilities … but they had an ability to adapt to environments that are quite diverse.”\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Print available models\n",
        "        print(\"Available summarization models:\")\n",
        "        summarizer = TextSummarizer()  # Initialize with default model\n",
        "        models = summarizer.get_available_models()\n",
        "        for i, model in enumerate(models):\n",
        "            print(f\"{i+1}. {model['name']} - {model['description']} ({model['size']})\")\n",
        "\n",
        "        print(\"\\nUsing default model:\", summarizer.model_name)\n",
        "\n",
        "        # Generate a concise summary\n",
        "        print(\"\\nGenerating concise summary...\")\n",
        "        concise_summary = summarizer.summarize(\n",
        "            sample_text,\n",
        "            max_length=75,\n",
        "            min_length=30,\n",
        "            style=\"concise\"\n",
        "        )\n",
        "\n",
        "        # Generate a detailed summary\n",
        "        print(\"Generating detailed summary...\")\n",
        "        detailed_summary = summarizer.summarize(\n",
        "            sample_text,\n",
        "            style=\"detailed\"\n",
        "        )\n",
        "\n",
        "        # Generate bullet points\n",
        "        print(\"Generating bullet points...\")\n",
        "        bullet_points = summarizer.summarize(\n",
        "            sample_text,\n",
        "            style=\"bullet-points\"\n",
        "        )\n",
        "\n",
        "        # Evaluate the concise summary\n",
        "        print(\"Evaluating summary...\")\n",
        "        evaluation = summarizer.evaluate_summary(sample_text, concise_summary)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n--- ORIGINAL TEXT ---\")\n",
        "        print(textwrap.fill(sample_text, width=80))\n",
        "\n",
        "        print(\"\\n--- CONCISE SUMMARY ---\")\n",
        "        print(textwrap.fill(concise_summary, width=80))\n",
        "\n",
        "        print(\"\\n--- DETAILED SUMMARY ---\")\n",
        "        print(textwrap.fill(detailed_summary, width=80))\n",
        "\n",
        "        print(\"\\n--- BULLET POINTS ---\")\n",
        "        print(bullet_points)\n",
        "\n",
        "        print(\"\\n--- EVALUATION ---\")\n",
        "        for metric, score in evaluation.items():\n",
        "            print(f\"{metric}: {score:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"1. Make sure you have installed the required packages:\")\n",
        "        print(\"   pip install torch transformers\")\n",
        "        print(\"2. For evaluation metrics, install rouge:\")\n",
        "        print(\"   pip install rouge\")\n",
        "        print(\"3. If you have limited RAM, try using a smaller model:\")\n",
        "        print(\"   summarizer = TextSummarizer('t5-small')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cl78icVJEFn",
        "outputId": "4d165020-7ad2-400e-994a-32ddbd4b2276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available summarization models:\n",
            "Loading model: facebook/bart-large-cnn (this may take a minute on first run)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully. Using device: cpu\n",
            "1. facebook/bart-large-cnn - Good balance between quality and speed, trained on CNN/DM dataset (400MB)\n",
            "2. t5-small - Smaller and faster model, good for resource-constrained environments (242MB)\n",
            "3. google/pegasus-xsum - State-of-the-art model for extreme summarization (very concise) (2.2GB)\n",
            "4. philschmid/bart-large-cnn-samsum - Fine-tuned for dialogue summarization (400MB)\n",
            "5. sshleifer/distilbart-cnn-12-6 - Distilled version of BART, faster with similar quality (306MB)\n",
            "\n",
            "Using default model: facebook/bart-large-cnn\n",
            "\n",
            "Generating concise summary...\n",
            "Generating detailed summary...\n",
            "Generating bullet points...\n",
            "Evaluating summary...\n",
            "Rouge package not found. Using basic evaluation.\n",
            "\n",
            "--- ORIGINAL TEXT ---\n",
            "     An ancient jawbone dredged from the Taiwanese seabed has revealed new\n",
            "insights into the appearance and sweeping geographic range of an enigmatic human\n",
            "species called the Denisovans.      The fossil was discovered by fishers\n",
            "trawling the Penghu Channel off Taiwan and is thought to be the most complete\n",
            "fossil that has been genetically identified as Denisovan. The male individual,\n",
            "who lived at least 10,000 years ago, had a strong jaw and very large, powerful\n",
            "molars.      “From a tooth or a small bone fragment, there’s the mystery of\n",
            "their appearance,” said Prof Enrico Cappellini, of the University of Copenhagen,\n",
            "a co-senior author on the paper. A Denisovan jaw discovered in Tibet had begun\n",
            "to fill in this picture, and the latest discovery adds to the evidence of a\n",
            "prominent jaw with huge teeth.      “Now we have a richer image,” Cappellini\n",
            "said. “Of course it would be good to have a skull and the rest of the skeleton,\n",
            "but it’s a step forward.”      The fossil has been dated to one of two glacial\n",
            "periods when the channel is known to have been above sea level, either between\n",
            "10,000 and 70,000 years ago or between 130,000 and 190,000 years ago.     The\n",
            "discovery reveals an impressive geographic range for the ancient species, which\n",
            "lived at the same time as – and interbred with – modern humans and Neanderthals.\n",
            "The first Denisovan fossils, identified through analysis of ancient DNA, came\n",
            "from a cave in Siberia and comprised just a finger fragment and a few teeth.\n",
            "Since then, further discoveries show Denisovans also weathered the incredibly\n",
            "harsh conditions of the high-altitude Tibetan plateau, where temperatures can\n",
            "plunge to -30C. By contrast, in south-east Asia they would have lived alongside\n",
            "water buffaloes in a balmy climate.      “These are climate and environmental\n",
            "conditions that are quite different,” Cappellini said. “The cold environment in\n",
            "Siberia, high altitude in Tibet. We cannot infer anything of their cognitive\n",
            "abilities … but they had an ability to adapt to environments that are quite\n",
            "diverse.”\n",
            "\n",
            "--- CONCISE SUMMARY ---\n",
            "The fossil was discovered by fishers trawling the Penghu Channel off Taiwan. It\n",
            "is thought to be the most complete fossil that has been genetically identified\n",
            "as Denisovan. The male individual had a strong jaw and very large, powerful\n",
            "molars.\n",
            "\n",
            "--- DETAILED SUMMARY ---\n",
            "The fossil was discovered by fishers trawling the Penghu Channel off Taiwan. It\n",
            "is thought to be the most complete fossil that has been genetically identified\n",
            "as Denisovan. The male individual, who lived at least 10,000 years ago, had a\n",
            "strong jaw and very large, powerful molars. The first Denisovan fossils,\n",
            "identified through analysis of ancient DNA, came from a cave in Siberia and\n",
            "comprised just a finger fragment and a few teeth. Since then, further\n",
            "discoveries show Denisovans also weathered the incredibly harsh conditions of\n",
            "the high-altitude Tibetan plateau.\n",
            "\n",
            "--- BULLET POINTS ---\n",
            "• The fossil was discovered by fishers trawling the Penghu Channel off Taiwan.\n",
            "• It is thought to be the most complete fossil that has been genetically identified as Denisovan.\n",
            "• The male individual had a strong jaw and very large, powerful molars.\n",
            "\n",
            "--- EVALUATION ---\n",
            "word_precision: 0.944\n",
            "word_recall: 0.162\n",
            "word_f1: 0.276\n",
            "overall_score: 0.276\n"
          ]
        }
      ]
    }
  ]
}